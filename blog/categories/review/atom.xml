<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Review | なんでもやってみるのが良いと思う]]></title>
  <link href="http://stepxstep.org/blog/categories/review/atom.xml" rel="self"/>
  <link href="http://stepxstep.org/"/>
  <updated>2014-11-04T02:08:59+09:00</updated>
  <id>http://stepxstep.org/</id>
  <author>
    <name><![CDATA[H.Tokida]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[SoftlayerのslコマンドのDocker化]]></title>
    <link href="http://stepxstep.org/blog/2014/11/04/sl-docker/"/>
    <updated>2014-11-04T01:46:00+09:00</updated>
    <id>http://stepxstep.org/blog/2014/11/04/sl-docker</id>
    <content type="html"><![CDATA[<p>SoftLayerのSLコマンドをDocker化しました。
単にDockerhubのAutomated Buildで遊んでみたかっただけなのですが・・</p>

<p>特に理由ないのですが各Linuxサーバに Python + SoftLayer-Cli 入れるのと Docker 入れるのとどっちがいいかなと思った場合にアプリケーションに影響なく利用できそうなのはDockerかなと。</p>

<p>作ってみてローカルのMacbookで利用しているのですが便利です。
これまでVagrant経由でVM起動してそのなかでゴニョゴニョしていました。今回のDockerにしてもboot2dockerで結局のところ似たような感じなのですが<code>config setup</code>で複数のSoftLayer環境を用意する際や、VersionUpをした際に簡単に分離して管理できるのは楽でした。 (まあ.dotfile切り替えるだけなら別の手段もあるかと思いますが)</p>

<h1 id="section">利用方法</h1>

<h2 id="section-1">想定</h2>

<p>このツールは、IBM社のクラウドサービス「SoftLayer」のCLIコマンドである sl コマンドを利用することが出来ます。
本番機などであまりslコマンド及び前提であるpythonを導入したくない場合。Dockerを利用うすることでOS側に影響なくコマンドを利用することが出来ます。 このDockerのImageは470MByte前後のサイズに成ります。</p>

<h2 id="section-2">設定</h2>

<p>以下のコマンドで設定ファイルをコンテナ内部に作ります。</p>

<p><code>$ docker run  -ti tokida/softlayer-cli config setup</code></p>

<p>通常のslコマンド同様にUsernameとAPIキーを設定して下さい。
次に、コンテナを自分用としてローカルにcommitしておきます。</p>

<p><code>$ docker restart 993ae5495011</code></p>

<p>一度コンテナをRestartします。</p>

<p><code>$ docker commit -m "my account" 993ae5495011 tokida/softlayer-cli:my</code></p>

<p>(数字はコンテナIDで <code>docker ps -a</code>で参照すること)
ここではTagに<code>my</code>をつけています。</p>

<h2 id="section-3">利用方法</h2>

<p><code>$ docker run  -ti tokida/softlayer-cli:my  vs list</code></p>

<p>という形で利用することが出来ます。長いのでシェルでエリアス等をしておくと良いかと思います。</p>

<p><code>$ alias sl="docker run  -ti tokida/softlayer-cli:my"</code></p>

<p>こうしておくと普段通り <code>sl</code> コマンドが利用できます。</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Vxlan on Softlayer]]></title>
    <link href="http://stepxstep.org/blog/2014/11/01/vxlan-on-softlayer/"/>
    <updated>2014-11-01T00:44:00+09:00</updated>
    <id>http://stepxstep.org/blog/2014/11/01/vxlan-on-softlayer</id>
    <content type="html"><![CDATA[<ul id="markdown-toc">
  <li><a href="#section">これは何？</a></li>
  <li><a href="#section-1">環境</a></li>
  <li><a href="#subnet">同一Subnet内</a></li>
  <li><a href="#vlan">異なるVLAN間</a></li>
</ul>

<h1 id="section">これは何？</h1>

<p>SoftLayerのネットワークはマルチキャストが使えるのでvxlanが動くのかを確認。</p>

<ol>
  <li>同一のSubnet内で利用可能か</li>
  <li>同一のvlan間で利用可能か（これは今回オーダー出来ず）</li>
  <li>異なるVLAN間（VLAN Spaning = on )で利用可能か</li>
</ol>

<p>というところを確認する。</p>

<h1 id="section-1">環境</h1>

<p>今回用意したのはダラスデータセンター内に仮想サーバを2つ起動している状態。プライベートIPで割り当てられているのは同一のSubnet上で登録されている。OSはUbuntu14.04を利用。</p>

<p>仮想サーバは以下のコマンドで作成する。</p>

<pre><code>sl vs create --datacenter=dal01 --cpu=1 --memory=1024 --os=UBUNTU_14_64 --domain=sl.com --hostname=test01 --hourly --san --disk=25,10 --key=mainkey --postinstall=https://gist.githubusercontent.com/tokida/5b58831c0d94ce7b25f2/raw/bootstrap4Ubuntu.sh
</code></pre>

<p>作成した結果は以下の感じになりました。同じSubnetになっていますね。違うSubnetに付け直したかったら動するのがいいのかな？:</p>

<pre><code>:.........:............:...............:.......:........:................:.............:....................:...........:
:    id   : datacenter :      host     : cores : memory :   primary_ip   :  backend_ip : active_transaction :   owner   :
:.........:............:...............:.......:........:................:.............:....................:...........:
: 6743772 :   dal01    : test01.sl.com :   1   :   1G   : 67.228.***.*** : 10.17.93.45 :         -          : *****     :
: 6816622 :   dal01    : test02.sl.com :   1   :   1G   : 67.228.***.*** : 10.17.93.46 :         -          : *****     :
:.........:............:...............:.......:........:................:.............:....................:...........:
</code></pre>

<h1 id="subnet">同一Subnet内</h1>

<p>以下のコマンドによりvxlanを設定する。今回は内部eth0に対して作成する</p>

<pre><code>$ ip link add vxlan0 type vxlan id 42 group 239.1.1.1 dev eth0
$ ip link set up vxlan0
$ ip a add 192.168.42.3/24 dev vxlan0
$ root@test02:~# ip -d link show vxlan0
4: vxlan0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1450 qdisc noqueue state UNKNOWN mode DEFAULT group default
    link/ether 2e:eb:95:ed:97:9d brd ff:ff:ff:ff:ff:ff promiscuity 0
    vxlan id 42 group 239.1.1.1 dev eth0 port 32768 61000 ageing 300
</code></pre>

<p>test01は、<code>192.168.42.2</code>を付与、test02は、<code>192.168.43.3</code>を付与します。</p>

<p>Pingを実行すると</p>

<pre><code>root@test02:~# ping 192.168.42.2
PING 192.168.42.2 (192.168.42.2) 56(84) bytes of data.
64 bytes from 192.168.42.2: icmp_seq=1 ttl=64 time=0.957 ms
64 bytes from 192.168.42.2: icmp_seq=2 ttl=64 time=0.377 ms
</code></pre>

<p>通信ができていることがわかります。マルチキャスト動いているようですね。</p>

<h1 id="vlan">異なるVLAN間</h1>

<p>3台目のサーバをサンノゼに作成します。この場合Subetが違いまたVLANが違う環境となります。VLANスパニングを有効にしているのでこのダラス⇔サンノゼ通信ができるようになっています。</p>

<pre><code>:.........:............:...............:.......:........:................:.............:....................:...........:
:    id   : datacenter :      host     : cores : memory :   primary_ip   :  backend_ip : active_transaction :   owner   :
:.........:............:...............:.......:........:................:.............:....................:...........:
: 6743772 :   dal01    : test01.sl.com :   1   :   1G   : 67.228.***.*** : 10.17.93.45 :         -          : ********* :
: 6816622 :   dal01    : test02.sl.com :   1   :   1G   : 67.228.***.*** : 10.17.93.46 :         -          : ********* :
: 6816998 :   sjc01    : test03.sl.com :   1   :   1G   : 158.85.***.*** : 10.89.0.170 :                    : ********* :
:.........:............:...............:.......:........:................:.............:....................:...........:
</code></pre>

<p>先ほど同様に設定を行い <code>192.168.42.4/24</code>としました。</p>

<pre><code>root@test01:~# ping 192.168.42.4
PING 192.168.42.4 (192.168.42.4) 56(84) bytes of data.
From 192.168.42.2 icmp_seq=1 Destination Host Unreachable
From 192.168.42.2 icmp_seq=2 Destination Host Unreachable
From 192.168.42.2 icmp_seq=3 Destination Host Unreachable
From 192.168.42.2 icmp_seq=4 Destination Host Unreachable
From 192.168.42.2 icmp_seq=5 Destination Host Unreachable
</code></pre>

<p>あわよくば動けば面白いなと思ったけど残念ながらこちらは通信出来ない模様ですね。さすがにDataCenterをまたいでマルチキャストが許可されていないのでしょうか。</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[SoftLayerに帯域保証型iSCSI/NASが登場したので試してみた]]></title>
    <link href="http://stepxstep.org/blog/2014/09/17/consistentstorage/"/>
    <updated>2014-09-17T20:40:00+09:00</updated>
    <id>http://stepxstep.org/blog/2014/09/17/consistentstorage</id>
    <content type="html"><![CDATA[<p>SoftLayerでの新機能で「帯域保証型ブロックデバイス」が登場しました。
1月ほど前からAPIには登場していたのですがようやく正式にリリースされた感じですね。</p>

<h1 id="section">これは何？</h1>

<p>SoftLayerの帯域保証型のブロックストレージを購入して簡単にfioでパフォーマンス測定してみましょう。
対象としては普段から利用していることもあり仮想サーバ上からUbuntu14.04を利用したいと思います。</p>

<ul>
  <li>容量 20Gから12TByteまで</li>
  <li>帯域保証 100IOPS から 6000IOPSまで</li>
  <li>iSCSI(Block Storage)とNAS(File Storage)の2つのインターフェースで提供</li>
</ul>

<h1 id="section-1">オーダーから設定まで</h1>

<h2 id="section-2">オーダ方法</h2>

<p>オーダ時のオプションで利用OSを選択するようになっていますね。
管理ポータルから、<code>Storage</code>→<code>Block Storage</code>→<code>Order Consistent Performance</code>を選択しましょう。</p>

<p><img src="/images/2014-09-17-iscsi.png" alt="iscsi" /></p>

<p>ここでの費用は、「ディスクサイズ」＋「IOPS」となるようです。この画像の場合、<code>20 GB 100 to 1000 IOPS</code>というは20GのiSCSIディスクはIOPS
として100から1000までが指定可能ということになります。一番安い、20G 100IOPSをオーダしてみたいと思います。</p>

<h2 id="section-3">設定方法</h2>

<p>購入したデバイス（ここでは<code>SL01SL29*****-1 (20 GB)</code>のような名称)の詳細を確認します。
iSCSIデバイスらしく<code>Authorized Hosts</code>という項目があるので利用したいサーバを指定します、すると「Username」「Password」「Host IQN」「Device Type」が該当のホスト向けに表示されます。</p>

<p>画面を見る限り以前までのSnapshotなどの機能はなさそうです（残念！）</p>

<p>今回のiSCSIはマルチパスで構成されているようなので<code>multipath-tools</code>を導入して利用します。</p>

<p><code>
apt-get install multipath-tools
apt-get install open-iscsi
</code></p>

<p>iSCSIの設定を行います。<code>/etc/iscsi/initiatorname.iscsi</code>のファイルにIQNを設定します。</p>

<p><code>
InitiatorName= "value-from-the-SL-Portal"
</code></p>

<p>次に<code>/etc/iscsi/iscsid.conf</code>を以下の内容を記載します。</p>

<p><code>
node.session.auth.authmethod = CHAP
node.session.auth.username = "Username-value-from-SL-Portal"
node.session.auth.password = "Password-value-from-SL-Portal"
discovery.sendtargets.auth.authmethod = CHAP
discovery.sendtargets.auth.username = "Username-value-from-SL-Portal"
discovery.sendtargets.auth.password = "Password-value-from-SL-Portal"
</code></p>

<p>設定が終わったら <code>/etc/init.d/open-iscsi restart</code>で再起動を行います。</p>

<p><code>
root@ansibletower:~# iscsiadm -m discovery -t sendtargets -p 10.2.174.111
10.2.174.111:3260,1036 iqn.1992-08.com.netapp:hkg0201
10.2.174.102:3260,1035 iqn.1992-08.com.netapp:hkg0201
</code></p>

<p>iSCSIを探してみると2つのIPで見つけることが出来ました。先ほど書いたようにMultipathでの接続が前提のようですね。</p>

<p><code>
root@ansibletower:~#  iscsiadm -m node --login
Logging in to [iface: default, target: iqn.1992-08.com.netapp:hkg0201, portal: 10.2.174.111,3260] (multiple)
Logging in to [iface: default, target: iqn.1992-08.com.netapp:hkg0201, portal: 10.2.174.102,3260] (multiple)
Login to [iface: default, target: iqn.1992-08.com.netapp:hkg0201, portal: 10.2.174.111,3260] successful.
Login to [iface: default, target: iqn.1992-08.com.netapp:hkg0201, portal: 10.2.174.102,3260] successful.
root@ansibletower:~#
root@ansibletower:~#
root@ansibletower:~# iscsiadm -m session -o show
tcp: [1] 10.2.174.111:3260,1036 iqn.1992-08.com.netapp:hkg0201
tcp: [2] 10.2.174.102:3260,1035 iqn.1992-08.com.netapp:hkg0201
</code></p>

<p>この作業により<code>/dev/sda</code>と<code>/dev/sdb</code>としてiSCSIが認識されます。また先ほど導入したmultipathを確認すると</p>

<p><code>
root@ansibletower:~# multipath -l
3600a0980383030525324464331596470 dm-0 NETAPP,LUN C-Mode
size=20G features='1 queue_if_no_path' hwhandler='0' wp=rw
|-+- policy='round-robin 0' prio=-1 status=active
| `- 0:0:0:186 sdb  8:16   active undef running
`-+- policy='round-robin 0' prio=-1 status=enabled
  `- 1:0:0:186 sda  8:0    active undef running
</code></p>

<p>このようにFailover型でデバイスが登録されているのがわかります。</p>

<p><code>
root@ansibletower:~#  fdisk -l | grep /dev/mapper
ディスク /dev/sdb は正常なパーティションテーブルを含んでいません
ディスク /dev/sda は正常なパーティションテーブルを含んでいません
ディスク /dev/mapper/3600a0980383030525324464331596470 は正常なパーティションテーブルを含んでいません
Disk /dev/mapper/3600a0980383030525324464331596470: 21.5 GB, 21474836480 bytes
</code></p>

<p>device mapper経由では上記のデバイスとして認識されています。これ以降はこのデバイスに対してfdiskが有効なのでパーティションを作成して利用することに成ります。</p>

<p><code>
root@ansibletower:~# lsblk
NAME                                       MAJ:MIN RM   SIZE RO TYPE  MOUNTPOINT
sda                                          8:0    0    20G  0 disk
└─3600a0980383030525324464331596470 (dm-0) 252:0    0    20G  0 mpath
sdb                                          8:16   0    20G  0 disk
└─3600a0980383030525324464331596470 (dm-0) 252:0    0    20G  0 mpath
xvda                                       202:0    0   100G  0 disk
├─xvda1                                    202:1    0   243M  0 part  /boot
└─xvda2                                    202:2    0  99.8G  0 part  /
xvdb                                       202:16   0     2G  0 disk
└─xvdb1                                    202:17   0     2G  0 part  [SWAP]
</code></p>

<p>fdiskでパーティションを作成し、カーネルにデバイスの変更を通知してxfsformatを行います。</p>

<p>&#8220;`
root@ansibletower:~# fdisk /dev/mapper/3600a0980383030525324464331596470</p>

<p>root@ansibletower:~# partprobe
Error: /dev/sda: ディスクラベルが認識できません。
Error: /dev/sdb: ディスクラベルが認識できません。
Error: /dev/mapper/3600a0980383030525324464331596470p1: ディスクラベルが認識できません。
root@ansibletower:~# mkfs.xfs /dev/mapper/3600a0980383030525324464331596470p1
meta-data=/dev/mapper/3600a0980383030525324464331596470p1 isize=256    agcount=16, agsize=327663 blks
         =                       sectsz=4096  attr=2, projid32bit=0
data     =                       bsize=4096   blocks=5242608, imaxpct=25
         =                       sunit=1      swidth=16 blks
naming   =version 2              bsize=4096   ascii-ci=0
log      =internal log           bsize=4096   blocks=2560, version=2
         =                       sectsz=4096  sunit=1 blks, lazy-count=1
realtime =none                   extsz=4096   blocks=0, rtextents=0
&#8220;`</p>

<p>実際にmountしてみる。恒久的に利用する場合には別途定義を行って下さい。</p>

<p><code>
root@ansibletower:~# mkdir /mnt/data1
root@ansibletower:~# mount /dev/mapper/3600a0980383030525324464331596470p1 /mnt/data1
</code></p>

<h1 id="iops">IOPS試験の実施</h1>

<p>簡単にするためにgistにスクリプトを置いています</p>

<p><code>
$ apt-get install fio
$ apt-get install curl
$ export DISK=/mnt/data1 ; curl -s https://gist.githubusercontent.com/tokida/090eaa6475e58b4368c0/raw/fio_test.sh | sh
</code></p>

<table>
  <thead>
    <tr>
      <th style="text-align: left">Benchmark</th>
      <th style="text-align: right">Bandwiddh</th>
      <th style="text-align: right">IOPS</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: left">4k, sequential read</td>
      <td style="text-align: right"> </td>
      <td style="text-align: right">147</td>
    </tr>
    <tr>
      <td style="text-align: left">4k, sequential write</td>
      <td style="text-align: right"> </td>
      <td style="text-align: right">165</td>
    </tr>
    <tr>
      <td style="text-align: left">4k, randam read</td>
      <td style="text-align: right"> </td>
      <td style="text-align: right">107</td>
    </tr>
    <tr>
      <td style="text-align: left">4k, randam write</td>
      <td style="text-align: right"> </td>
      <td style="text-align: right">103</td>
    </tr>
    <tr>
      <td style="text-align: left">32m, sequential read</td>
      <td style="text-align: right">51.871MB/s</td>
      <td style="text-align: right"> </td>
    </tr>
    <tr>
      <td style="text-align: left">32m, sequential write</td>
      <td style="text-align: right">12.221MB/s</td>
      <td style="text-align: right"> </td>
    </tr>
  </tbody>
</table>

<p>参考までに同じ試験項目でローカルディスク(100G SAN)を実施した場合</p>

<table>
  <thead>
    <tr>
      <th style="text-align: left">Benchmark</th>
      <th style="text-align: right">Bandwiddh</th>
      <th style="text-align: right">IOPS</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: left">4k, sequential read</td>
      <td style="text-align: right"> </td>
      <td style="text-align: right">27362</td>
    </tr>
    <tr>
      <td style="text-align: left">4k, sequential write</td>
      <td style="text-align: right"> </td>
      <td style="text-align: right">26305</td>
    </tr>
    <tr>
      <td style="text-align: left">4k, randam read</td>
      <td style="text-align: right"> </td>
      <td style="text-align: right">14188</td>
    </tr>
    <tr>
      <td style="text-align: left">4k, randam write</td>
      <td style="text-align: right"> </td>
      <td style="text-align: right">22233</td>
    </tr>
    <tr>
      <td style="text-align: left">32m, sequential read</td>
      <td style="text-align: right">114.070MB/s</td>
      <td style="text-align: right"> </td>
    </tr>
    <tr>
      <td style="text-align: left">32m, sequential write</td>
      <td style="text-align: right">113.188MB/s</td>
      <td style="text-align: right"> </td>
    </tr>
  </tbody>
</table>

<h1 id="section-4">まとめ</h1>

<p>今回オーダしたのは100IOPSのBlock Storageになりますが、確かに100前後で制御されているように見えます。帯域はこれがMaxなのか分かりませんがこのあたりも今後確認していきたいところですね。</p>

<p>良かった点は、iSCSIがMultipathになった点でしょうか、とはいえ手軽さはNFS(FileStorage)での利用ですかね。また帯域が保証されているので6000IOPSを複数束ねてRaid0構成も出来るかもしれませんね。ただし結構費用が高いのが辛いかもしれないですが。12TByte(6000IOPS)の場合 $1,200+$720になるので $0.16/GByte となりますね。</p>

<p><img src="/images/2014-09-17-iscsi02.png" alt="os" /></p>

<p>このパラメータが何を意味しているのか調べていないのですが<code>AIX</code>とかありますね！</p>

<p>悪かった点は、以前のiSCSIが抽象化された利便性の高いデバイスだったのですが今回はかなり素のiSCSIデバイスとしての利用になります。とくにSnapshotが使えないのは残念ですね。今回の場合にはこのiSCSIデバイスのバックアップをなにか考えなければいけません。12Tとか考えるとかなり面倒な感じですね。</p>

<h1 id="section-5">参考</h1>

<ul>
  <li><a href="http://knowledgelayer.softlayer.com/procedure/accessing-block-storage-consistent-performance-linux">Accessing Block Storage Consistent Performance on Linux</a></li>
  <li><a href="https://gist.githubusercontent.com/tokida/090eaa6475e58b4368c0/raw/738cec3a3dfa7816d65882c5d45d513e4a8cc160/fio_test.sh">FIO試験スクリプト</a></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[XMLRPCに大量にアクセスされている場合の回避策]]></title>
    <link href="http://stepxstep.org/blog/2014/08/25/xml-rpc-ddos/"/>
    <updated>2014-08-25T11:28:00+09:00</updated>
    <id>http://stepxstep.org/blog/2014/08/25/xml-rpc-ddos</id>
    <content type="html"><![CDATA[<p>さてCPU使用率が100%になったりApacheのデーモンが大量に発生したりした場合どんな原因が考えられますか？
最近担当しているサーバからMemoryがいっぱいだよ、CPUがいっぱいだよとNew Relic経由でメールが届いたと思ったら触れなくなっていました。</p>

<p>調べてみるとhttpdが大量に起動してメモリがなくなりフォークしたプロセスがKillされている状態でした。標準の設定でhttpd.confを書いているのでそもそも大量にコネクションがあった場合（今回のようなケース）はメモリが足りないなという状態だったので計算をしみたところ一つあたり57MByteもメモリを使っているという富豪な設定になっていました。ApacheでLoadModuleされすぎです。（CentOSの標準パッケージの問題だと思いますが）</p>

<p>さて、そのようなことは横においておくとして今回の原因は xml-rpc.php に対して不特定の大量のアクセスが発生指定していることから起こっています。</p>

<p>以下の例は暫定で<code>xmlrpc.php</code>を削除しています。</p>

<p><code>
88.150.160.81 - - [23/Aug/2014:11:24:30 -0500] "POST /xmlrpc.php HTTP/1.1" 404 286 "-" "-"
172.245.37.220 - - [23/Aug/2014:11:24:31 -0500] "POST /xmlrpc.php HTTP/1.1" 404 286 "-" "-"
96.8.126.60 - - [23/Aug/2014:11:24:31 -0500] "POST /xmlrpc.php HTTP/1.1" 404 286 "-" "-"
109.104.118.82 - - [23/Aug/2014:11:24:31 -0500] "POST /xmlrpc.php HTTP/1.1" 404 286 "-" "-"
88.150.160.82 - - [23/Aug/2014:11:24:32 -0500] "POST /xmlrpc.php HTTP/1.1" 404 286 "-" "-"
172.245.221.237 - - [23/Aug/2014:11:24:32 -0500] "POST /xmlrpc.php HTTP/1.1" 404 286 "-" "-"
185.17.151.106 - - [23/Aug/2014:11:24:32 -0500] "POST /xmlrpc.php HTTP/1.1" 404 286 "-" "-"
217.78.5.180 - - [23/Aug/2014:11:24:32 -0500] "POST /xmlrpc.php HTTP/1.1" 404 286 "-" "-"
192.3.43.160 - - [23/Aug/2014:11:24:32 -0500] "POST /xmlrpc.php HTTP/1.1" 404 286 "-" "-"
96.8.123.95 - - [23/Aug/2014:11:24:34 -0500] "POST /xmlrpc.php HTTP/1.1" 404 286 "-" "-"
217.78.5.165 - - [23/Aug/2014:11:24:37 -0500] "POST /xmlrpc.php HTTP/1.1" 404 286 "-" "-"
^C
</code></p>

<h3 id="section">対策</h3>

<p>幾つかの対策が考えられます。</p>

<ul>
  <li>XMLRPCの機能を無効化する（Wordpress3.5以降はディフォルトで有効になっているためソースを変更する必要がありそうです／VersionUp等の際にまた忘れる可能性があるので今回は選択しません）。</li>
  <li>プラグインで無効化（プラグインで無効化してもPHPが動くためCPUリソースは食べそうです）／これはこれで導入しておきましょう。</li>
  <li>接続可能なIPアドレスを制限する（今回はこれ）</li>
</ul>

<p>今回はこの3番目のアドレスで制限をしてみたいと思います。このサーバは、Public側とPrivate側に2つのネットワークインターフェースを持っているサーバとなっています。また設定についてはApacheだったので以下のようなディレクティブを追加することで対応が出来ます。</p>

<h3 id="section-1">設定内容</h3>

<ul>
  <li><code>httpd.conf</code>に以下を追記する。xmlrpc.php　は 10.0.0.0　からのアクセスのみ許可にする。</li>
</ul>

<p><code>
&lt;Files xmlrpc.php&gt;
    order deny,allow
    deny from all
    allow from 10.0.0.0/255.0.0.0
&lt;/Files&gt;
</code></p>

<p>これでPrivateNetworki(10.0.0.0)側のみになりそれ以外は403でエラーになります。</p>

<p><code>
172.245.37.225 - - [23/Aug/2014:11:35:55 -0500] "POST /xmlrpc.php HTTP/1.1" 403 290 "-" "-"
109.104.118.117 - - [23/Aug/2014:11:35:56 -0500] "POST /xmlrpc.php HTTP/1.1" 403 290 "-" "-"
109.104.118.82 - - [23/Aug/2014:11:35:56 -0500] "POST /xmlrpc.php HTTP/1.1" 403 290 "-" "-"
198.23.154.30 - - [23/Aug/2014:11:35:57 -0500] "POST /xmlrpc.php HTTP/1.1" 403 290 "-" "-"
109.104.118.117 - - [23/Aug/2014:11:35:57 -0500] "POST /xmlrpc.php HTTP/1.1" 403 290 "-" "-"
217.78.5.166 - - [23/Aug/2014:11:35:58 -0500] "POST /xmlrpc.php HTTP/1.1" 403 290 "-" "-"
109.104.118.131 - - [23/Aug/2014:11:35:58 -0500] "POST /xmlrpc.php HTTP/1.1" 403 290 "-" "-"
96.8.125.21 - - [23/Aug/2014:11:35:59 -0500] "POST /xmlrpc.php HTTP/1.1" 403 290 "-" "-"
217.78.5.165 - - [23/Aug/2014:11:35:59 -0500] "POST /xmlrpc.php HTTP/1.1" 403 290 "-" "-"
109.104.118.77 - - [23/Aug/2014:11:36:00 -0500] "POST /xmlrpc.php HTTP/1.1" 403 290 "-" "-"
^C
</code></p>

<h3 id="section-2">まとめ</h3>

<p>今回リソースはNewRelicで見ていましたがログまでは保管していなかったため問題が発生している際に状況を調べるのが面倒でした。というかCPU100%の状態を解消しないと見ることが出来ませんでした。キチンとFluentd等で別のログサーバにデータを転送しておきたいと思います。</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Serfを試してみる]]></title>
    <link href="http://stepxstep.org/blog/2014/08/22/first_serf/"/>
    <updated>2014-08-22T01:30:00+09:00</updated>
    <id>http://stepxstep.org/blog/2014/08/22/first_serf</id>
    <content type="html"><![CDATA[<ul id="markdown-toc">
  <li><a href="#section">はじめに</a></li>
  <li><a href="#section-1">導入</a></li>
  <li><a href="#section-2">起動</a>    <ul>
      <li><a href="#mdns">mDNSを利用したクラスタへのログイン</a></li>
    </ul>
  </li>
  <li><a href="#section-3">使い方</a>    <ul>
      <li><a href="#eventhandler">最初に各サーバに最初のEventHandlerのシェルを配置する</a></li>
    </ul>
  </li>
  <li><a href="#section-4">まとめ</a></li>
  <li><a href="#section-5">参考</a></li>
</ul>

<h1 id="section">はじめに</h1>

<p>Software Design 2014/09号に「Serf・Consul入門」が載っていたのでそれを参考に少し動かしてみました。また記事を書かれている @zenbutu さんのQiita上でのまとめを参考にしています。</p>

<ul>
  <li><a href="http://qiita.com/zembutsu/items/aaffab81f9d5b60d7ecc">SerfとConsulの記事まとめ - Qiita</a></li>
  <li><a href="http://www.amazon.co.jp/exec/obidos/ASIN/B00M7AWOJQ/ref=nosim?tag=maftracking33806-22&amp;linkCode=ure&amp;creative=6339">Software Design (ソフトウェア デザイン) 2014年 09月号</a></li>
</ul>

<h1 id="section-1">導入</h1>

<p><a href="http://www.serfdom.io/downloads.html">http://www.serfdom.io/downloads.html</a></p>

<p>上記から各サーバにあったものを落とす。
ここではUbuntu14.04を2台で試すので</p>

<p><a href="https://dl.bintray.com/mitchellh/serf/0.6.3_linux_amd64.zip">https://dl.bintray.com/mitchellh/serf/0.6.3_linux_amd64.zip</a></p>

<p>を利用する</p>

<pre><code>$ apt-get install unzip
$ wget https://dl.bintray.com/mitchellh/serf/0.6.3_linux_amd64.zip
$ unzip 0.6.3_linux_amd64.zip
$ cp ./serf /usr/bin/serf
</code></pre>

<p>バージョンの確認</p>

<pre><code>$ serf version
Serf v0.6.3
Agent Protocol: 4 (Understands back to: 2)
</code></pre>

<h1 id="section-2">起動</h1>

<p>バックグラウンドでの起動</p>

<pre><code>root@serf1:~# serf agent &amp;
[1] 10094
root@serf1:~# ==&gt; Starting Serf agent...
==&gt; Starting Serf agent RPC...
==&gt; Serf agent running!
         Node name: 'serf1'
         Bind addr: '0.0.0.0:7946'
          RPC addr: '127.0.0.1:7373'
         Encrypted: false
          Snapshot: false
           Profile: lan

==&gt; Log data will now stream in as it occurs:

    2014/08/21 18:31:15 [INFO] agent: Serf agent starting
    2014/08/21 18:31:15 [INFO] serf: EventMemberJoin: serf1 10.114.16.197
    2014/08/21 18:31:16 [INFO] agent: Received event: member-join
</code></pre>

<p>エージェントの確認／クラスタの状態の確認</p>

<pre><code>root@serf1:~# serf members
    2014/08/21 18:32:13 [INFO] agent.ipc: Accepted client: 127.0.0.1:44401
serf1  10.114.16.197:7946  alive
</code></pre>

<p>2台目のサーバで、上記のSerf導入後以下を実施</p>

<pre><code>root@serf2$ serf agent &amp;
root@serf1$ serf join 10.114.16.197 
    2014/08/21 18:35:05 [INFO] agent.ipc: Accepted client: 127.0.0.1:48145
    2014/08/21 18:35:05 [INFO] agent: joining: [10.114.16.197] replay: false
    2014/08/21 18:35:05 [INFO] serf: EventMemberJoin: serf1 10.114.16.197
    2014/08/21 18:35:05 [INFO] agent: joined: 1 nodes
Successfully joined cluster by contacting 1 nodes.
root@serf2:~#     2014/08/21 18:35:06 [INFO] agent: Received event: member-join
</code></pre>

<p>1台目でMemberの確認</p>

<pre><code>root@serf1:~# serf members
    2014/08/21 18:36:12 [INFO] agent.ipc: Accepted client: 127.0.0.1:44421
serf1  10.114.16.197:7946  alive
serf2  10.114.16.196:7946  alive
</code></pre>

<p>止める</p>

<pre><code>root@serf1:~# serf leave
    2014/08/21 18:44:31 [INFO] agent.ipc: Accepted client: 127.0.0.1:44457
    2014/08/21 18:44:31 [INFO] agent.ipc: Graceful leave triggered
    2014/08/21 18:44:31 [INFO] agent: requesting graceful leave from Serf
    2014/08/21 18:44:31 [INFO] serf: EventMemberLeave: serf1 10.114.16.197
    2014/08/21 18:44:31 [INFO] agent: requesting serf shutdown
    2014/08/21 18:44:31 [INFO] agent: shutdown complete
    2014/08/21 18:44:31 [WARN] agent: Serf shutdown detected, quitting
Error leaving: client closed
[1]+  終了                  serf agent
</code></pre>

<h2 id="mdns">mDNSを利用したクラスタへのログイン</h2>

<p>serfサーバをJoinするのにIPアドレスなどでしてしないでディスカバリーする。今回の環境はSoftLayerなのでマルチキャストDNS(mDNS)が通信できるためディスカバリーすることが出来ます。</p>

<p>先に2台とも停止しておく<code>serf leave</code></p>

<pre><code>root@serf1:~# ==&gt; Starting Serf agent...
==&gt; Starting Serf agent RPC...
==&gt; Serf agent running!
         Node name: 'serf1'
         Bind addr: '0.0.0.0:7946'
          RPC addr: '127.0.0.1:7373'
         Encrypted: false
          Snapshot: false
           Profile: lan
      mDNS cluster: serf

==&gt; Log data will now stream in as it occurs:

    2014/08/21 18:46:50 [INFO] agent: Serf agent starting
    2014/08/21 18:46:50 [INFO] serf: EventMemberJoin: serf1 10.114.16.197
    2014/08/21 18:46:51 [INFO] agent: joining: [10.114.16.197:7946] replay: false
    2014/08/21 18:46:51 [INFO] agent: joined: 1 nodes
    2014/08/21 18:46:51 [INFO] agent.mdns: Joined 1 hosts
    2014/08/21 18:46:51 [INFO] agent: Received event: member-join

root@serf1:~# serf members
    2014/08/21 18:47:01 [INFO] agent.ipc: Accepted client: 127.0.0.1:44467
serf1  10.114.16.197:7946  alive
</code></pre>

<p>次にserf2側でも実行</p>

<pre><code>root@serf2:~# ==&gt; Starting Serf agent...
==&gt; Starting Serf agent RPC...
==&gt; Serf agent running!
         Node name: 'serf2'
         Bind addr: '0.0.0.0:7946'
          RPC addr: '127.0.0.1:7373'
         Encrypted: false
          Snapshot: false
           Profile: lan
      mDNS cluster: serf

==&gt; Log data will now stream in as it occurs:

    2014/08/21 18:47:23 [INFO] agent: Serf agent starting
    2014/08/21 18:47:23 [INFO] serf: EventMemberJoin: serf2 10.114.16.196
    2014/08/21 18:47:23 [INFO] agent: joining: [10.114.16.196:7946 10.114.16.197:7946] replay: false
    2014/08/21 18:47:23 [INFO] serf: EventMemberJoin: serf1 10.114.16.197
    2014/08/21 18:47:23 [INFO] agent: joined: 2 nodes
    2014/08/21 18:47:23 [INFO] agent.mdns: Joined 2 hosts
    2014/08/21 18:47:24 [INFO] agent: Received event: member-join

root@serf2:~# serf members
    2014/08/21 18:47:31 [INFO] agent.ipc: Accepted client: 127.0.0.1:48185
serf2  10.114.16.196:7946  alive
serf1  10.114.16.197:7946  alive
</code></pre>

<h1 id="section-3">使い方</h1>

<p>このserfでは<code>Event</code>というものを使って処理ができるようになるのが特徴。その際の処理は<code>Event Handler</code>と言うものが担っている。</p>

<p>EventHander自体は、スクリプト等の実行コマンドで各サーバ上に事前に用意しておく必要がある。したがって複数の処理をしたい場合などはGitなど経由でダウンロードする仕組みを組み込んでいくほうが良いと思われる。</p>

<h2 id="eventhandler">最初に各サーバに最初のEventHandlerのシェルを配置する</h2>

<p>以下の内容で ~/event.sh を作成</p>

<p><figure class='code'><figcaption><span>event.sh </span></figcaption>
<div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
</pre></td><td class='code'><pre><code class='sh'><span class='line'><span class="c">#!/bin/sh &lt;/p&gt;</span>
</span><span class='line'>
</span><span class='line'>&lt;p&gt;LOG<span class="o">=</span>event.log&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;echo “—–” » <span class="nv">$LOG</span>
</span><span class='line'>hostname » <span class="nv">$LOG</span>
</span><span class='line'>date » <span class="nv">$LOG</span>
</span><span class='line'>env <span class="p">|</span>grep “^SERF” » <span class="nv">$LOG</span>
</span></code></pre></td></tr></table></div></figure></p>

<p>次に各serfを起動する。一旦<code>serf leave</code>で抜けておいて各サーバで以下のコマンドを実行する。</p>

<p>serf1側を、<code>-role=server</code>として serf2側を <code>-role=agent</code> として起動します。 </p>

<pre><code>serf agent -event-handler=./event.sh -discover=serf -role=&lt;ROLE&gt; &amp;
</code></pre>

<p>これにより、何らかのイベントが発行される都度に <code>event.sh</code>が実行されます。その際の書きだされる内容は</p>

<pre><code>root@serf1:~# tail -f event.log
SERF_EVENT=member-join
SERF_SELF_ROLE=
SERF_SELF_NAME=serf1
-----
serf1
2014年  8月 21日 木曜日 19:16:36 JST
SERF_EVENT=member-join
SERF_SELF_ROLE=server
SERF_TAG_ROLE=server
SERF_SELF_NAME=serf1
    2014/08/21 19:17:08 [INFO] serf: EventMemberLeave: serf2 10.114.16.196
    2014/08/21 19:17:09 [INFO] agent: Received event: member-leave
-----
serf1
2014年  8月 21日 木曜日 19:17:09 JST
SERF_EVENT=member-leave
SERF_SELF_ROLE=server
SERF_TAG_ROLE=server
SERF_SELF_NAME=serf1
    2014/08/21 19:17:27 [INFO] serf: EventMemberJoin: serf2 10.114.16.196
    2014/08/21 19:17:28 [INFO] agent: Received event: member-join
-----
serf1
2014年  8月 21日 木曜日 19:17:28 JST
SERF_EVENT=member-join
SERF_SELF_ROLE=server
SERF_TAG_ROLE=server
SERF_SELF_NAME=serf1
</code></pre>

<p>次に</p>

<pre><code>root@serf2:~# tail -f event.log
SERF_EVENT=member-join
SERF_SELF_ROLE=
SERF_SELF_NAME=serf2
-----
serf2
2014年  8月 21日 木曜日 19:17:28 JST
SERF_EVENT=member-join
SERF_SELF_ROLE=agent
SERF_TAG_ROLE=agent
SERF_SELF_NAME=serf2
</code></pre>

<p>つまり、このEventHandlerのスクリプトをゴニョゴニョすると色々出来ますね。という話です。任意のイベント名を指定して実行してみます。serf2から<code>event TEST</code>を実行します。</p>

<pre><code>root@serf2:~# serf event TEST
</code></pre>

<p>この時、serf1、serf2で同時に event.sh が実行されます。すると</p>

<pre><code>serf1
2014年  8月 21日 木曜日 19:27:16 JST
SERF_EVENT=user
SERF_SELF_ROLE=server
SERF_TAG_ROLE=server
SERF_USER_LTIME=3
SERF_SELF_NAME=serf1
SERF_USER_EVENT=TEST
</code></pre>

<p>のようなイベントな環境変数で実行されます。
これを利用して、XXXサーバ機能として Role/Tag/Serf_Nameを駆使してサーバを特定して特定の処理を実施すると良いことになりますね。</p>

<h1 id="section-4">まとめ</h1>

<p>ざっくり言えば、用意しておいスクリプトをクラスタ環境で実行することが出来る。実行はサーバが起動した際等にmember-joinした際に動くなど、複数のサーバ間で何かの処理をしたい時に同時に実行することが出来るのが面白い。</p>

<p>スクリプトへ渡されるものは、SERF＿EVENTの場合 msmber-* は、標準入力で引き渡されます。その際にはホスト名やIPアドレスを取得することが出来ます。またEventの場合には、引数で指定したデータが渡ります。</p>

<p><a href="http://qiita.com/foostan/items/5d61595b1b331a73b8c9">Dockerで試す、はじめてのSerf - Qiita</a></p>

<p>によるとEventHandlerをイベント事に指定することも出来るようなので色々出来そうです。反面、スクリプトを各サーバに用意しておく必要がありこの部分をプロビジョニング時にうまいことする必要もありそうですね。</p>

<h1 id="section-5">参考</h1>

<ul>
  <li><a href="http://pocketstudio.jp/log3/2014/04/01/serf_event_handlers/">イベントハンドラを整理してみる</a></li>
  <li><a href="http://blog.livedoor.jp/sonots/archives/35397486.html">正月休みだし Serf 触ってみた - sonots:blog</a></li>
</ul>

]]></content>
  </entry>
  
</feed>
